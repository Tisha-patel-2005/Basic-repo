{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNyNZz2EkiTjk6x+yUj/6xO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tisha-patel-2005/Basic-repo/blob/main/main1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "Pql1u603iBA1"
      },
      "outputs": [],
      "source": [
        "# Create project directories\n",
        "\n",
        "!mkdir -p BirdAudioProject\n",
        "!mkdir -p BirdAudioProject/static/css\n",
        "!mkdir -p BirdAudioProject/static/js\n",
        "!mkdir -p BirdAudioProject/static/data\n",
        "!mkdir -p BirdAudioProject/static/audio\n",
        "!mkdir -p BirdAudioProject/static/images\n",
        "!mkdir -p BirdAudioProject/templates\n",
        "!mkdir -p BirdAudioProject/models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "\n",
        "!pip install flask librosa tensorflow scikit-learn matplotlib numpy pandas flask-ngrok pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFaDBURziyNp",
        "outputId": "4dc9bfdc-e388-4f8a-f8b4-aeac7bd035e3"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: flask-ngrok in /usr/local/lib/python3.11/dist-packages (0.0.25)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.13.1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this in your Google Colab notebook\n",
        "\n",
        "%%writefile data_collector.py\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "def download_xeno_canto_data(species_list, output_dir, max_recordings_per_species=10):\n",
        "    \"\"\"\n",
        "    Download bird audio recordings from Xeno-Canto API.\n",
        "\n",
        "    Args:\n",
        "        species_list (list): List of bird species to download\n",
        "        output_dir (str): Directory to save downloaded recordings\n",
        "        max_recordings_per_species (int): Maximum number of recordings per species\n",
        "    \"\"\"\n",
        "    base_url = \"https://www.xeno-canto.org/api/2/recordings\"\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for species in species_list:\n",
        "        print(f\"Downloading recordings for {species}...\")\n",
        "\n",
        "        # Create species directory\n",
        "        species_dir = os.path.join(output_dir, species.replace(\" \", \"_\"))\n",
        "        os.makedirs(species_dir, exist_ok=True)\n",
        "\n",
        "        # Query Xeno-Canto API\n",
        "        query = f\"?query={species} q:A\"  # q:A means high quality recordings\n",
        "        response = requests.get(base_url + query)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            recordings = data.get('recordings', [])\n",
        "\n",
        "            # Limit number of recordings\n",
        "            recordings = recordings[:min(len(recordings), max_recordings_per_species)]\n",
        "\n",
        "            print(f\"Found {len(recordings)} recordings for {species}\")\n",
        "\n",
        "            # Download each recording\n",
        "            for i, recording in enumerate(recordings):\n",
        "                try:\n",
        "                    # Get download link\n",
        "                    file_url = recording.get('file')\n",
        "                    if not file_url:\n",
        "                        continue\n",
        "\n",
        "                    # Extract file name and extension\n",
        "                    file_name = f\"{species.replace(' ', '_')}_{i+1}.mp3\"\n",
        "                    file_path = os.path.join(species_dir, file_name)\n",
        "\n",
        "                    # Download file\n",
        "                    print(f\"Downloading {file_name}...\")\n",
        "                    audio_response = requests.get(file_url)\n",
        "\n",
        "                    if audio_response.status_code == 200:\n",
        "                        with open(file_path, 'wb') as f:\n",
        "                            f.write(audio_response.content)\n",
        "                        print(f\"Downloaded {file_name}\")\n",
        "                    else:\n",
        "                        print(f\"Failed to download {file_name}\")\n",
        "\n",
        "                    # Add a small delay to avoid overwhelming the server\n",
        "                    time.sleep(1)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error downloading recording: {e}\")\n",
        "\n",
        "            # Add a delay between species to be nice to the API\n",
        "            time.sleep(3)\n",
        "        else:\n",
        "            print(f\"Failed to query Xeno-Canto API for {species}: {response.status_code}\")\n",
        "\n",
        "# List of bird species to download\n",
        "species_list = [\n",
        "    \"American Robin\",\n",
        "    \"Northern Cardinal\",\n",
        "    \"Blue Jay\",\n",
        "    \"Barn Owl\",\n",
        "    \"Red-tailed Hawk\"\n",
        "]\n",
        "\n",
        "# Download data (10 recordings per species)\n",
        "download_xeno_canto_data(species_list, \"bird_audio_data\", max_recordings_per_species=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WkYxs5ki_tS",
        "outputId": "ea389890-f579-4cc3-990d-f8651313296c"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing data_collector.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install requests pandas tqdm\n",
        "\n",
        "# Run the data collection script\n",
        "!python data_collector.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZDyJUSumI50",
        "outputId": "ba30b11d-e77f-4ffe-b5fc-245483c4cb5e"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading recordings for American Robin...\n",
            "Found 10 recordings for American Robin\n",
            "Downloading American_Robin_1.mp3...\n",
            "Downloaded American_Robin_1.mp3\n",
            "Downloading American_Robin_2.mp3...\n",
            "Downloaded American_Robin_2.mp3\n",
            "Downloading American_Robin_3.mp3...\n",
            "Downloaded American_Robin_3.mp3\n",
            "Downloading American_Robin_4.mp3...\n",
            "Downloaded American_Robin_4.mp3\n",
            "Downloading American_Robin_5.mp3...\n",
            "Downloaded American_Robin_5.mp3\n",
            "Downloading American_Robin_6.mp3...\n",
            "Downloaded American_Robin_6.mp3\n",
            "Downloading American_Robin_7.mp3...\n",
            "Downloaded American_Robin_7.mp3\n",
            "Downloading American_Robin_8.mp3...\n",
            "Downloaded American_Robin_8.mp3\n",
            "Downloading American_Robin_9.mp3...\n",
            "Downloaded American_Robin_9.mp3\n",
            "Downloading American_Robin_10.mp3...\n",
            "Downloaded American_Robin_10.mp3\n",
            "Downloading recordings for Northern Cardinal...\n",
            "Found 10 recordings for Northern Cardinal\n",
            "Downloading Northern_Cardinal_1.mp3...\n",
            "Downloaded Northern_Cardinal_1.mp3\n",
            "Downloading Northern_Cardinal_2.mp3...\n",
            "Downloaded Northern_Cardinal_2.mp3\n",
            "Downloading Northern_Cardinal_3.mp3...\n",
            "Downloaded Northern_Cardinal_3.mp3\n",
            "Downloading Northern_Cardinal_4.mp3...\n",
            "Downloaded Northern_Cardinal_4.mp3\n",
            "Downloading Northern_Cardinal_5.mp3...\n",
            "Downloaded Northern_Cardinal_5.mp3\n",
            "Downloading Northern_Cardinal_6.mp3...\n",
            "Downloaded Northern_Cardinal_6.mp3\n",
            "Downloading Northern_Cardinal_7.mp3...\n",
            "Downloaded Northern_Cardinal_7.mp3\n",
            "Downloading Northern_Cardinal_8.mp3...\n",
            "Downloaded Northern_Cardinal_8.mp3\n",
            "Downloading Northern_Cardinal_9.mp3...\n",
            "Downloaded Northern_Cardinal_9.mp3\n",
            "Downloading Northern_Cardinal_10.mp3...\n",
            "Downloaded Northern_Cardinal_10.mp3\n",
            "Downloading recordings for Blue Jay...\n",
            "Found 10 recordings for Blue Jay\n",
            "Downloading Blue_Jay_1.mp3...\n",
            "Downloaded Blue_Jay_1.mp3\n",
            "Downloading Blue_Jay_2.mp3...\n",
            "Downloaded Blue_Jay_2.mp3\n",
            "Downloading Blue_Jay_3.mp3...\n",
            "Downloaded Blue_Jay_3.mp3\n",
            "Downloading Blue_Jay_4.mp3...\n",
            "Downloaded Blue_Jay_4.mp3\n",
            "Downloading Blue_Jay_5.mp3...\n",
            "Downloaded Blue_Jay_5.mp3\n",
            "Downloading Blue_Jay_6.mp3...\n",
            "Downloaded Blue_Jay_6.mp3\n",
            "Downloading Blue_Jay_7.mp3...\n",
            "Downloaded Blue_Jay_7.mp3\n",
            "Downloading Blue_Jay_8.mp3...\n",
            "Downloaded Blue_Jay_8.mp3\n",
            "Downloading Blue_Jay_9.mp3...\n",
            "Downloaded Blue_Jay_9.mp3\n",
            "Downloading Blue_Jay_10.mp3...\n",
            "Downloaded Blue_Jay_10.mp3\n",
            "Downloading recordings for Barn Owl...\n",
            "Found 10 recordings for Barn Owl\n",
            "Downloading Barn_Owl_1.mp3...\n",
            "Downloaded Barn_Owl_1.mp3\n",
            "Downloading Barn_Owl_2.mp3...\n",
            "Downloaded Barn_Owl_2.mp3\n",
            "Downloading Barn_Owl_3.mp3...\n",
            "Downloaded Barn_Owl_3.mp3\n",
            "Downloading Barn_Owl_4.mp3...\n",
            "Downloaded Barn_Owl_4.mp3\n",
            "Downloading Barn_Owl_5.mp3...\n",
            "Downloaded Barn_Owl_5.mp3\n",
            "Downloading Barn_Owl_6.mp3...\n",
            "Downloaded Barn_Owl_6.mp3\n",
            "Downloading Barn_Owl_7.mp3...\n",
            "Downloaded Barn_Owl_7.mp3\n",
            "Downloading Barn_Owl_8.mp3...\n",
            "Downloaded Barn_Owl_8.mp3\n",
            "Downloading Barn_Owl_9.mp3...\n",
            "Downloaded Barn_Owl_9.mp3\n",
            "Downloading Barn_Owl_10.mp3...\n",
            "Downloaded Barn_Owl_10.mp3\n",
            "Downloading recordings for Red-tailed Hawk...\n",
            "Found 10 recordings for Red-tailed Hawk\n",
            "Downloading Red-tailed_Hawk_1.mp3...\n",
            "Downloaded Red-tailed_Hawk_1.mp3\n",
            "Downloading Red-tailed_Hawk_2.mp3...\n",
            "Downloaded Red-tailed_Hawk_2.mp3\n",
            "Downloading Red-tailed_Hawk_3.mp3...\n",
            "Downloaded Red-tailed_Hawk_3.mp3\n",
            "Downloading Red-tailed_Hawk_4.mp3...\n",
            "Downloaded Red-tailed_Hawk_4.mp3\n",
            "Downloading Red-tailed_Hawk_5.mp3...\n",
            "Downloaded Red-tailed_Hawk_5.mp3\n",
            "Downloading Red-tailed_Hawk_6.mp3...\n",
            "Downloaded Red-tailed_Hawk_6.mp3\n",
            "Downloading Red-tailed_Hawk_7.mp3...\n",
            "Downloaded Red-tailed_Hawk_7.mp3\n",
            "Downloading Red-tailed_Hawk_8.mp3...\n",
            "Downloaded Red-tailed_Hawk_8.mp3\n",
            "Downloading Red-tailed_Hawk_9.mp3...\n",
            "Downloaded Red-tailed_Hawk_9.mp3\n",
            "Downloading Red-tailed_Hawk_10.mp3...\n",
            "Downloaded Red-tailed_Hawk_10.mp3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a directory for bird images\n",
        "!mkdir -p BirdAudioProject/static/images\n",
        "\n",
        "# Download bird images using wget\n",
        "!wget -O BirdAudioProject/static/images/american_robin.jpg \"https://upload.wikimedia.org/wikipedia/commons/b/b8/Turdus-migratorius-002.jpg\"\n",
        "!wget -O BirdAudioProject/static/images/northern_cardinal.jpg \"https://upload.wikimedia.org/wikipedia/commons/d/da/Cardinal.jpg\"\n",
        "!wget -O BirdAudioProject/static/images/blue_jay.jpg \"https://tse2.mm.bing.net/th?id=OIP.LW7owKtD8WTiQPJjH-R1mwHaE7&pid=Api&P=0&h=180.jpg\"\n",
        "!wget -O BirdAudioProject/static/images/barn_owl.jpg \"https://tse2.mm.bing.net/th?id=OIP.d5OM94ifQJ3NPa54gJYuSAHaEo&pid=Api&P=0&h=180.jpg\"\n",
        "!wget -O BirdAudioProject/static/images/red_tailed_hawk.jpg \"https://upload.wikimedia.org/wikipedia/commons/5/51/Buteo_jamaicensis_-John_Heinz_National_Wildlife_Refuge_at_Tinicum%2C_Pennsylvania%2C_USA-8.jpg\"\n",
        "!wget -O BirdAudioProject/static/images/about-image.jpg \"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/Everglades_National_Park_Birds.jpg/1280px-Everglades_National_Park_Birds.jpg\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1V0AdpdntJ9R",
        "outputId": "f43dbff3-5dcf-4954-f51e-ce35e47b071d"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-09 14:46:27--  https://upload.wikimedia.org/wikipedia/commons/b/b8/Turdus-migratorius-002.jpg\n",
            "Resolving upload.wikimedia.org (upload.wikimedia.org)... 198.35.26.112, 2620:0:863:ed1a::2:b\n",
            "Connecting to upload.wikimedia.org (upload.wikimedia.org)|198.35.26.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 368966 (360K) [image/jpeg]\n",
            "Saving to: ‘BirdAudioProject/static/images/american_robin.jpg’\n",
            "\n",
            "\r          BirdAudio   0%[                    ]       0  --.-KB/s               \rBirdAudioProject/st 100%[===================>] 360.32K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2025-04-09 14:46:27 (4.43 MB/s) - ‘BirdAudioProject/static/images/american_robin.jpg’ saved [368966/368966]\n",
            "\n",
            "--2025-04-09 14:46:27--  https://upload.wikimedia.org/wikipedia/commons/d/da/Cardinal.jpg\n",
            "Resolving upload.wikimedia.org (upload.wikimedia.org)... 198.35.26.112, 2620:0:863:ed1a::2:b\n",
            "Connecting to upload.wikimedia.org (upload.wikimedia.org)|198.35.26.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 59435 (58K) [image/jpeg]\n",
            "Saving to: ‘BirdAudioProject/static/images/northern_cardinal.jpg’\n",
            "\n",
            "BirdAudioProject/st 100%[===================>]  58.04K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-04-09 14:46:28 (1.78 MB/s) - ‘BirdAudioProject/static/images/northern_cardinal.jpg’ saved [59435/59435]\n",
            "\n",
            "--2025-04-09 14:46:28--  https://tse2.mm.bing.net/th?id=OIP.LW7owKtD8WTiQPJjH-R1mwHaE7&pid=Api&P=0&h=180.jpg\n",
            "Resolving tse2.mm.bing.net (tse2.mm.bing.net)... 150.171.27.10, 150.171.28.10, 2620:1ec:33::10, ...\n",
            "Connecting to tse2.mm.bing.net (tse2.mm.bing.net)|150.171.27.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23715 (23K) [image/jpeg]\n",
            "Saving to: ‘BirdAudioProject/static/images/blue_jay.jpg’\n",
            "\n",
            "BirdAudioProject/st 100%[===================>]  23.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-04-09 14:46:28 (103 MB/s) - ‘BirdAudioProject/static/images/blue_jay.jpg’ saved [23715/23715]\n",
            "\n",
            "--2025-04-09 14:46:28--  https://tse2.mm.bing.net/th?id=OIP.d5OM94ifQJ3NPa54gJYuSAHaEo&pid=Api&P=0&h=180.jpg\n",
            "Resolving tse2.mm.bing.net (tse2.mm.bing.net)... 150.171.27.10, 150.171.28.10, 2620:1ec:33::10, ...\n",
            "Connecting to tse2.mm.bing.net (tse2.mm.bing.net)|150.171.27.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22917 (22K) [image/jpeg]\n",
            "Saving to: ‘BirdAudioProject/static/images/barn_owl.jpg’\n",
            "\n",
            "BirdAudioProject/st 100%[===================>]  22.38K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-04-09 14:46:28 (268 MB/s) - ‘BirdAudioProject/static/images/barn_owl.jpg’ saved [22917/22917]\n",
            "\n",
            "--2025-04-09 14:46:28--  https://upload.wikimedia.org/wikipedia/commons/5/51/Buteo_jamaicensis_-John_Heinz_National_Wildlife_Refuge_at_Tinicum%2C_Pennsylvania%2C_USA-8.jpg\n",
            "Resolving upload.wikimedia.org (upload.wikimedia.org)... 198.35.26.112, 2620:0:863:ed1a::2:b\n",
            "Connecting to upload.wikimedia.org (upload.wikimedia.org)|198.35.26.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1667458 (1.6M) [image/jpeg]\n",
            "Saving to: ‘BirdAudioProject/static/images/red_tailed_hawk.jpg’\n",
            "\n",
            "BirdAudioProject/st 100%[===================>]   1.59M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-04-09 14:46:28 (12.6 MB/s) - ‘BirdAudioProject/static/images/red_tailed_hawk.jpg’ saved [1667458/1667458]\n",
            "\n",
            "--2025-04-09 14:46:28--  https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/Everglades_National_Park_Birds.jpg/1280px-Everglades_National_Park_Birds.jpg\n",
            "Resolving upload.wikimedia.org (upload.wikimedia.org)... 198.35.26.112, 2620:0:863:ed1a::2:b\n",
            "Connecting to upload.wikimedia.org (upload.wikimedia.org)|198.35.26.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-04-09 14:46:28 ERROR 404: Not Found.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the data directory structure\n",
        "!mkdir -p BirdAudioProject/data/bird_audio\n",
        "!mkdir -p BirdAudioProject/models\n",
        "\n",
        "# Move the downloaded data to the project structure\n",
        "!cp -r bird_audio_data/* BirdAudioProject/data/bird_audio/\n",
        "\n",
        "# Split data into training and testing sets (80% training, 20% testing)\n",
        "!mkdir -p BirdAudioProject/data/train_bird_audio\n",
        "!mkdir -p BirdAudioProject/data/test_bird_audio"
      ],
      "metadata": {
        "id": "pCSbIbPFzQud"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Run this script to split the data\n",
        "%%writefile split_data.py\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "def split_data(source_dir, train_dir, test_dir, test_ratio=0.2):\n",
        "    \"\"\"\n",
        "    Split data into training and testing sets.\n",
        "\n",
        "    Args:\n",
        "        source_dir (str): Source directory containing species folders\n",
        "        train_dir (str): Directory to save training data\n",
        "        test_dir (str): Directory to save testing data\n",
        "        test_ratio (float): Ratio of data to use for testing\n",
        "    \"\"\"\n",
        "    # Create output directories\n",
        "    os.makedirs(train_dir, exist_ok=True)\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "    # Get species directories\n",
        "    species_dirs = [d for d in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, d))]\n",
        "\n",
        "    for species in species_dirs:\n",
        "        # Create species directories in train and test\n",
        "        train_species_dir = os.path.join(train_dir, species)\n",
        "        test_species_dir = os.path.join(test_dir, species)\n",
        "        os.makedirs(train_species_dir, exist_ok=True)\n",
        "        os.makedirs(test_species_dir, exist_ok=True)\n",
        "\n",
        "        # Get audio files\n",
        "        species_dir = os.path.join(source_dir, species)\n",
        "        audio_files = [f for f in os.listdir(species_dir) if f.endswith(('.wav', '.mp3', '.ogg'))]\n",
        "\n",
        "        # Shuffle files\n",
        "        random.shuffle(audio_files)\n",
        "\n",
        "        # Calculate split\n",
        "        test_size = max(1, int(len(audio_files) * test_ratio))\n",
        "        test_files = audio_files[:test_size]\n",
        "        train_files = audio_files[test_size:]\n",
        "\n",
        "        # Copy files\n",
        "        for file in train_files:\n",
        "            shutil.copy(os.path.join(species_dir, file), os.path.join(train_species_dir, file))\n",
        "\n",
        "        for file in test_files:\n",
        "            shutil.copy(os.path.join(species_dir, file), os.path.join(test_species_dir, file))\n",
        "\n",
        "        print(f\"{species}: {len(train_files)} training files, {len(test_files)} testing files\")\n",
        "\n",
        "# Split the data\n",
        "split_data(\"BirdAudioProject/data/bird_audio\",\n",
        "           \"BirdAudioProject/data/train_bird_audio\",\n",
        "           \"BirdAudioProject/data/test_bird_audio\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b8PvFkNtXuc",
        "outputId": "dc9686d1-e5e0-4ee6-b1e3-6f0ab9210093"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting split_data.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Third cell: Run the script\n",
        "!python split_data.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dra8d76hzW3_",
        "outputId": "0da52c56-706f-47ff-b436-6d57c9bda115"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:werkzeug: * Detected change in '/content/BirdAudioProject/BirdAudioProject/split_data.py', reloading\n",
            "Red-tailed_Hawk: 8 training files, 2 testing files\n",
            "INFO:werkzeug: * Restarting with stat\n",
            "2025-04-09 14:46:32.410173: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744209992.435741   12525 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744209992.443579   12525 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Northern_Cardinal: 8 training files, 2 testing files\n",
            "Barn_Owl: 8 training files, 2 testing files\n",
            "Blue_Jay: 8 training files, 2 testing files\n",
            "American_Robin: 8 training files, 2 testing files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create audio_processor.py\n",
        "%%writefile BirdAudioProject/audio_processor.py\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class AudioProcessor:\n",
        "    def __init__(self, sample_rate=22050, n_mfcc=13, n_fft=2048, hop_length=512):\n",
        "        \"\"\"\n",
        "        Initialize the AudioProcessor with audio processing parameters.\n",
        "\n",
        "        Args:\n",
        "            sample_rate (int): Sample rate for audio processing\n",
        "            n_mfcc (int): Number of MFCC features to extract\n",
        "            n_fft (int): FFT window size\n",
        "            hop_length (int): Hop length for FFT\n",
        "        \"\"\"\n",
        "        self.sample_rate = sample_rate\n",
        "        self.n_mfcc = n_mfcc\n",
        "        self.n_fft = n_fft\n",
        "        self.hop_length = hop_length\n",
        "\n",
        "    def load_audio(self, file_path):\n",
        "        \"\"\"\n",
        "        Load audio file and return the signal and sample rate.\n",
        "\n",
        "        Args:\n",
        "            file_path (str): Path to the audio file\n",
        "\n",
        "        Returns:\n",
        "            tuple: (audio_signal, sample_rate)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            signal, sr = librosa.load(file_path, sr=self.sample_rate)\n",
        "            return signal, sr\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading audio file: {e}\")\n",
        "            return None, None\n",
        "\n",
        "    def extract_features(self, signal, feature_type='mfcc'):\n",
        "        \"\"\"\n",
        "        Extract audio features from the signal.\n",
        "\n",
        "        Args:\n",
        "            signal (numpy.ndarray): Audio signal\n",
        "            feature_type (str): Type of feature to extract ('mfcc', 'mel_spectrogram', etc.)\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Extracted features\n",
        "        \"\"\"\n",
        "        if signal is None:\n",
        "            return None\n",
        "\n",
        "        if feature_type == 'mfcc':\n",
        "            features = librosa.feature.mfcc(\n",
        "                y=signal,\n",
        "                sr=self.sample_rate,\n",
        "                n_mfcc=self.n_mfcc,\n",
        "                n_fft=self.n_fft,\n",
        "                hop_length=self.hop_length\n",
        "            )\n",
        "            return features\n",
        "\n",
        "        elif feature_type == 'mel_spectrogram':\n",
        "            features = librosa.feature.melspectrogram(\n",
        "                y=signal,\n",
        "                sr=self.sample_rate,\n",
        "                n_fft=self.n_fft,\n",
        "                hop_length=self.hop_length\n",
        "            )\n",
        "            return features\n",
        "\n",
        "        else:\n",
        "            print(f\"Feature type '{feature_type}' not supported\")\n",
        "            return None\n",
        "\n",
        "    def normalize_features(self, features):\n",
        "        \"\"\"\n",
        "        Normalize features to have zero mean and unit variance.\n",
        "\n",
        "        Args:\n",
        "            features (numpy.ndarray): Features to normalize\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Normalized features\n",
        "        \"\"\"\n",
        "        if features is None:\n",
        "            return None\n",
        "\n",
        "        mean = np.mean(features, axis=1, keepdims=True)\n",
        "        std = np.std(features, axis=1, keepdims=True)\n",
        "        normalized_features = (features - mean) / (std + 1e-10)\n",
        "        return normalized_features\n",
        "\n",
        "    def plot_features(self, features, feature_type='mfcc', save_path=None):\n",
        "        \"\"\"\n",
        "        Plot extracted features.\n",
        "\n",
        "        Args:\n",
        "            features (numpy.ndarray): Features to plot\n",
        "            feature_type (str): Type of feature ('mfcc', 'mel_spectrogram', etc.)\n",
        "            save_path (str, optional): Path to save the plot\n",
        "\n",
        "        Returns:\n",
        "            matplotlib.figure.Figure: Figure object\n",
        "        \"\"\"\n",
        "        if features is None:\n",
        "            return None\n",
        "\n",
        "        plt.figure(figsize=(10, 4))\n",
        "\n",
        "        if feature_type == 'mfcc':\n",
        "            librosa.display.specshow(\n",
        "                features,\n",
        "                x_axis='time',\n",
        "                sr=self.sample_rate,\n",
        "                hop_length=self.hop_length\n",
        "            )\n",
        "            plt.colorbar(format='%+2.0f dB')\n",
        "            plt.title('MFCC')\n",
        "\n",
        "        elif feature_type == 'mel_spectrogram':\n",
        "            librosa.display.specshow(\n",
        "                librosa.power_to_db(features, ref=np.max),\n",
        "                y_axis='mel',\n",
        "                x_axis='time',\n",
        "                sr=self.sample_rate,\n",
        "                hop_length=self.hop_length\n",
        "            )\n",
        "            plt.colorbar(format='%+2.0f dB')\n",
        "            plt.title('Mel spectrogram')\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path)\n",
        "\n",
        "        return plt.gcf()\n",
        "\n",
        "    def preprocess_for_model(self, file_path, fixed_length=100):\n",
        "        \"\"\"\n",
        "        Preprocess audio file for model input.\n",
        "\n",
        "        Args:\n",
        "            file_path (str): Path to the audio file\n",
        "            fixed_length (int): Fixed length for feature padding/truncation\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Preprocessed features ready for model input\n",
        "        \"\"\"\n",
        "        signal, sr = self.load_audio(file_path)\n",
        "        if signal is None:\n",
        "            return None\n",
        "\n",
        "        # Extract MFCC features\n",
        "        features = self.extract_features(signal, feature_type='mfcc')\n",
        "        if features is None:\n",
        "            return None\n",
        "\n",
        "        # Normalize features\n",
        "        normalized_features = self.normalize_features(features)\n",
        "\n",
        "        # Pad or truncate to fixed length\n",
        "        if normalized_features.shape[1] < fixed_length:\n",
        "            pad_width = fixed_length - normalized_features.shape[1]\n",
        "            normalized_features = np.pad(\n",
        "                normalized_features,\n",
        "                pad_width=((0, 0), (0, pad_width)),\n",
        "                mode='constant'\n",
        "            )\n",
        "        else:\n",
        "            normalized_features = normalized_features[:, :fixed_length]\n",
        "\n",
        "        # Reshape for model input (assuming CNN model)\n",
        "        normalized_features = normalized_features.reshape(1, normalized_features.shape[0], normalized_features.shape[1], 1)\n",
        "\n",
        "        return normalized_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xATWjIsbz9HN",
        "outputId": "34ae7598-a298-4776-b038-5aad91bf3eeb"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting BirdAudioProject/audio_processor.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model_trainer.py\n",
        "%%writefile BirdAudioProject/model_trainer.py\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "class ModelTrainer:\n",
        "    def __init__(self, input_shape=(128, 128, 1)):\n",
        "        \"\"\"\n",
        "        Initialize ModelTrainer.\n",
        "        Args:\n",
        "            input_shape (tuple): Shape of input features (Height, Width, Channels)\n",
        "        \"\"\"\n",
        "        self.input_shape = input_shape\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "        self.label_encoder = LabelEncoder()\n",
        "\n",
        "    def build_model(self, num_classes):\n",
        "        \"\"\"\n",
        "        Build and compile the CNN model.\n",
        "        \"\"\"\n",
        "        model = Sequential([\n",
        "            Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=self.input_shape),\n",
        "            BatchNormalization(),\n",
        "            MaxPooling2D((2, 2)),\n",
        "\n",
        "            Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "            BatchNormalization(),\n",
        "            MaxPooling2D((2, 2)),\n",
        "\n",
        "            Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "            BatchNormalization(),\n",
        "            MaxPooling2D((2, 2)),\n",
        "\n",
        "            GlobalAveragePooling2D(),\n",
        "            Dense(128, activation='relu'),\n",
        "            Dropout(0.4),\n",
        "            Dense(num_classes, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        self.model = model\n",
        "        return model\n",
        "\n",
        "    def train(self, X, y, epochs=50, batch_size=32, validation_split=0.2, save_best_model_path=None):\n",
        "        \"\"\"\n",
        "        Train the model with early stopping and optional model saving.\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not built. Call build_model first.\")\n",
        "\n",
        "        # Encode labels\n",
        "        encoded_y = self.label_encoder.fit_transform(y)\n",
        "        categorical_y = to_categorical(encoded_y)\n",
        "\n",
        "        # Split\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X, categorical_y, test_size=validation_split, random_state=42\n",
        "        )\n",
        "\n",
        "        # Callbacks\n",
        "        callbacks = [\n",
        "            EarlyStopping(patience=8, restore_best_weights=True, monitor='val_loss')\n",
        "        ]\n",
        "        if save_best_model_path:\n",
        "            callbacks.append(ModelCheckpoint(save_best_model_path, save_best_only=True, monitor='val_loss'))\n",
        "\n",
        "        # Train\n",
        "        history = self.model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_data=(X_val, y_val),\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        self.history = history\n",
        "        return history\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"\n",
        "        Evaluate the model on test data.\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not built or trained.\")\n",
        "\n",
        "        encoded_y = self.label_encoder.transform(y_test)\n",
        "        categorical_y = to_categorical(encoded_y)\n",
        "\n",
        "        loss, accuracy = self.model.evaluate(X_test, categorical_y, verbose=0)\n",
        "        print(f\"Test Loss: {loss:.4f}\")\n",
        "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "        return loss, accuracy\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels and probabilities.\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not trained.\")\n",
        "\n",
        "        probabilities = self.model.predict(X)\n",
        "        predicted_indices = np.argmax(probabilities, axis=1)\n",
        "        predicted_labels = self.label_encoder.inverse_transform(predicted_indices)\n",
        "        return predicted_labels, probabilities\n",
        "\n",
        "    def save_model(self, model_path, encoder_path=None):\n",
        "        \"\"\"\n",
        "        Save model and label encoder.\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not trained.\")\n",
        "\n",
        "        self.model.save(model_path)\n",
        "\n",
        "        if encoder_path:\n",
        "            with open(encoder_path, 'wb') as f:\n",
        "                pickle.dump(self.label_encoder, f)\n",
        "\n",
        "    def load_model(self, model_path, encoder_path=None):\n",
        "        \"\"\"\n",
        "        Load saved model and label encoder.\n",
        "        \"\"\"\n",
        "        self.model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "        if encoder_path and os.path.exists(encoder_path):\n",
        "            with open(encoder_path, 'rb') as f:\n",
        "                self.label_encoder = pickle.load(f)\n",
        "\n",
        "    def plot_training_history(self, save_path=None):\n",
        "        \"\"\"\n",
        "        Plot training history.\n",
        "        \"\"\"\n",
        "        if self.history is None:\n",
        "            raise ValueError(\"Model not trained yet.\")\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "        ax1.plot(self.history.history['accuracy'])\n",
        "        ax1.plot(self.history.history['val_accuracy'])\n",
        "        ax1.set_title('Accuracy')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_ylabel('Accuracy')\n",
        "        ax1.legend(['Train', 'Val'])\n",
        "\n",
        "        ax2.plot(self.history.history['loss'])\n",
        "        ax2.plot(self.history.history['val_loss'])\n",
        "        ax2.set_title('Loss')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.set_ylabel('Loss')\n",
        "        ax2.legend(['Train', 'Val'])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        if save_path:\n",
        "            plt.savefig(save_path)\n",
        "        return fig\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_v9q4Idt0GIl",
        "outputId": "c28511ac-1e96-42c7-81bc-2b864f3ea314"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing BirdAudioProject/model_trainer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train_model.py\n",
        "%%writefile BirdAudioProject/train_model.py\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from audio_processor import AudioProcessor\n",
        "from model_trainer import ModelTrainer\n",
        "\n",
        "def train_model_with_data(data_dir, output_dir):\n",
        "    \"\"\"\n",
        "    Train a bird species identification model with audio data.\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): Directory containing audio files organized by species\n",
        "        output_dir (str): Directory to save model and results\n",
        "    \"\"\"\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Initialize audio processor\n",
        "    processor = AudioProcessor(sample_rate=22050, n_mfcc=13)\n",
        "\n",
        "    # Lists to store features and labels\n",
        "    features_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    # Process each species directory\n",
        "    species_dirs = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
        "\n",
        "    print(f\"Found {len(species_dirs)} species directories\")\n",
        "\n",
        "    for species in species_dirs:\n",
        "        species_dir = os.path.join(data_dir, species)\n",
        "        print(f\"Processing {species}...\")\n",
        "\n",
        "        # Process each audio file in the species directory\n",
        "        audio_files = [f for f in os.listdir(species_dir) if f.endswith(('.wav', '.mp3', '.ogg'))]\n",
        "\n",
        "        for audio_file in audio_files:\n",
        "            file_path = os.path.join(species_dir, audio_file)\n",
        "\n",
        "            # Load and preprocess audio\n",
        "            signal, sr = processor.load_audio(file_path)\n",
        "            if signal is None:\n",
        "                continue\n",
        "\n",
        "            # Extract features\n",
        "            features = processor.extract_features(signal, feature_type='mfcc')\n",
        "            if features is None:\n",
        "                continue\n",
        "\n",
        "            # Normalize features\n",
        "            normalized_features = processor.normalize_features(features)\n",
        "\n",
        "            # Ensure consistent feature length (e.g., 100 time steps)\n",
        "            fixed_length = 100\n",
        "            if normalized_features.shape[1] < fixed_length:\n",
        "                pad_width = fixed_length - normalized_features.shape[1]\n",
        "                normalized_features = np.pad(\n",
        "                    normalized_features,\n",
        "                    pad_width=((0, 0), (0, pad_width)),\n",
        "                    mode='constant'\n",
        "                )\n",
        "            else:\n",
        "                normalized_features = normalized_features[:, :fixed_length]\n",
        "\n",
        "            # Add to lists\n",
        "            features_list.append(normalized_features)\n",
        "            labels_list.append(species)\n",
        "\n",
        "    # Convert lists to arrays\n",
        "    X = np.array(features_list)\n",
        "    y = np.array(labels_list)\n",
        "\n",
        "    # Reshape features for CNN input: (samples, height, width, channels)\n",
        "    X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
        "\n",
        "    print(f\"Feature shape: {X.shape}\")\n",
        "    print(f\"Number of samples: {len(y)}\")\n",
        "    print(f\"Unique species: {np.unique(y)}\")\n",
        "\n",
        "    # Initialize and build model\n",
        "    input_shape = (X.shape[1], X.shape[2], 1)\n",
        "    trainer = ModelTrainer(input_shape=input_shape)\n",
        "\n",
        "    num_classes = len(np.unique(y))\n",
        "    model = trainer.build_model(num_classes)\n",
        "\n",
        "    # Train model\n",
        "    history = trainer.train(X, y, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "    # Plot and save training history\n",
        "    trainer.plot_training_history(save_path=os.path.join(output_dir, 'training_history.png'))\n",
        "\n",
        "    # Save model and encoder\n",
        "    model_path = os.path.join(output_dir, 'bird_species_model.h5')\n",
        "    encoder_path = os.path.join(output_dir, 'label_encoder.pkl')\n",
        "    trainer.save_model(model_path, encoder_path)\n",
        "\n",
        "    print(f\"Model saved to {model_path}\")\n",
        "    print(f\"Label encoder saved to {encoder_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage\n",
        "    data_dir = \"path/to/bird_audio_data\"\n",
        "    output_dir = \"path/to/output\"\n",
        "    train_model_with_data(data_dir, output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0OVEdjv0KtL",
        "outputId": "d68858e8-cd87-4295-c03b-9e8bb77f82cf"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing BirdAudioProject/train_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create app.py\n",
        "%%writefile BirdAudioProject/app.py\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from flask import Flask, render_template, request, jsonify\n",
        "import tensorflow as tf\n",
        "from audio_processor import AudioProcessor\n",
        "import pickle\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import base64\n",
        "from werkzeug.utils import secure_filename\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load model and label encoder\n",
        "MODEL_PATH = 'models/bird_species_model.h5'\n",
        "ENCODER_PATH = 'models/label_encoder.pkl'\n",
        "UPLOAD_FOLDER = 'static/uploads'\n",
        "ALLOWED_EXTENSIONS = {'wav', 'mp3', 'ogg'}\n",
        "\n",
        "os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
        "\n",
        "# Initialize audio processor\n",
        "processor = AudioProcessor()\n",
        "\n",
        "# Load bird information\n",
        "def load_bird_info():\n",
        "    try:\n",
        "        with open('static/data/bird_info.json', 'r') as f:\n",
        "            return json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading bird info: {e}\")\n",
        "        return {}\n",
        "\n",
        "bird_info = load_bird_info()\n",
        "\n",
        "# Load model and encoder if they exist\n",
        "model = None\n",
        "label_encoder = None\n",
        "\n",
        "def load_model_and_encoder():\n",
        "    global model, label_encoder\n",
        "    try:\n",
        "        if os.path.exists(MODEL_PATH):\n",
        "            model = tf.keras.models.load_model(MODEL_PATH)\n",
        "            print(\"Model loaded successfully\")\n",
        "        else:\n",
        "            print(f\"Model file not found at {MODEL_PATH}\")\n",
        "\n",
        "        if os.path.exists(ENCODER_PATH):\n",
        "            with open(ENCODER_PATH, 'rb') as f:\n",
        "                label_encoder = pickle.load(f)\n",
        "            print(\"Label encoder loaded successfully\")\n",
        "        else:\n",
        "            print(f\"Label encoder file not found at {ENCODER_PATH}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model or encoder: {e}\")\n",
        "\n",
        "# Try to load model and encoder at startup\n",
        "try:\n",
        "    load_model_and_encoder()\n",
        "except Exception as e:\n",
        "    print(f\"Could not load model at startup: {e}\")\n",
        "\n",
        "def allowed_file(filename):\n",
        "    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return render_template('index.html')\n",
        "\n",
        "@app.route('/about')\n",
        "def about():\n",
        "    return render_template('about.html')\n",
        "\n",
        "@app.route('/identify', methods=['POST'])\n",
        "def identify_bird():\n",
        "    if 'audio_file' not in request.files:\n",
        "        return jsonify({'error': 'No file part'}), 400\n",
        "\n",
        "    file = request.files['audio_file']\n",
        "\n",
        "    if file.filename == '':\n",
        "        return jsonify({'error': 'No selected file'}), 400\n",
        "\n",
        "    if file and allowed_file(file.filename):\n",
        "        # Save the uploaded file\n",
        "        filename = secure_filename(file.filename)\n",
        "        file_path = os.path.join(UPLOAD_FOLDER, filename)\n",
        "        file.save(file_path)\n",
        "\n",
        "        # Check if model is loaded\n",
        "        if model is None or label_encoder is None:\n",
        "            try:\n",
        "                load_model_and_encoder()\n",
        "                if model is None or label_encoder is None:\n",
        "                    return jsonify({'error': 'Model not available'}), 500\n",
        "            except Exception as e:\n",
        "                return jsonify({'error': f'Error loading model: {str(e)}'}), 500\n",
        "\n",
        "        try:\n",
        "            # Process audio file\n",
        "            features = processor.preprocess_for_model(file_path)\n",
        "\n",
        "            if features is None:\n",
        "                return jsonify({'error': 'Failed to process audio file'}), 400\n",
        "\n",
        "            # Make prediction\n",
        "            probabilities = model.predict(features)[0]\n",
        "            predicted_index = np.argmax(probabilities)\n",
        "            predicted_species = label_encoder.inverse_transform([predicted_index])[0]\n",
        "            confidence = float(probabilities[predicted_index])\n",
        "\n",
        "            # Generate spectrogram for visualization\n",
        "            y, sr = librosa.load(file_path)\n",
        "            plt.figure(figsize=(10, 4))\n",
        "            S = librosa.feature.melspectrogram(y=y, sr=sr)\n",
        "            librosa.display.specshow(librosa.power_to_db(S, ref=np.max), y_axis='mel', x_axis='time')\n",
        "            plt.colorbar(format='%+2.0f dB')\n",
        "            plt.title('Mel spectrogram')\n",
        "\n",
        "            # Save plot to a bytes buffer\n",
        "            buf = io.BytesIO()\n",
        "            plt.savefig(buf, format='png')\n",
        "            buf.seek(0)\n",
        "            plt.close()\n",
        "\n",
        "            # Convert to base64 for embedding in HTML\n",
        "            spectrogram_b64 = base64.b64encode(buf.read()).decode('utf-8')\n",
        "\n",
        "            # Get bird info\n",
        "            bird_data = bird_info.get(predicted_species, {\n",
        "                'common_name': predicted_species,\n",
        "                'scientific_name': 'Unknown',\n",
        "                'description': 'No information available for this species.',\n",
        "                'image': '/static/images/placeholder.jpg'\n",
        "            })\n",
        "\n",
        "            # Return results\n",
        "            return jsonify({\n",
        "                'species': predicted_species,\n",
        "                'common_name': bird_data['common_name'],\n",
        "                'scientific_name': bird_data['scientific_name'],\n",
        "                'description': bird_data['description'],\n",
        "                'image': bird_data['image'],\n",
        "                'confidence': confidence,\n",
        "                'spectrogram': spectrogram_b64\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            return jsonify({'error': f'Error during identification: {str(e)}'}), 500\n",
        "\n",
        "    return jsonify({'error': 'Invalid file type'}), 400\n",
        "\n",
        "@app.route('/train', methods=['POST'])\n",
        "def train_model_endpoint():\n",
        "    # This would be a more complex endpoint for training the model\n",
        "    # In a real application, this might trigger a background job\n",
        "    return jsonify({'message': 'Training functionality not implemented in this demo'})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClevTyxL0R8I",
        "outputId": "4f9ea0c1-2ef9-419f-e31e-def9f3f73197"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting BirdAudioProject/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create layout.html\n",
        "%%writefile BirdAudioProject/templates/layout.html\n",
        "\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Bird Species Identifier</title>\n",
        "    <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='css/style.css') }}\">\n",
        "    <link href=\"https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap\" rel=\"stylesheet\">\n",
        "</head>\n",
        "<body>\n",
        "    <header>\n",
        "        <div class=\"container\">\n",
        "            <h1>Bird Species Identifier</h1>\n",
        "            <nav>\n",
        "                <ul>\n",
        "                    <li><a href=\"{{ url_for('index') }}\">Home</a></li>\n",
        "                    <li><a href=\"{{ url_for('about') }}\">About</a></li>\n",
        "                </ul>\n",
        "            </nav>\n",
        "        </div>\n",
        "    </header>\n",
        "\n",
        "    <main>\n",
        "        <div class=\"container\">\n",
        "            {% block content %}{% endblock %}\n",
        "        </div>\n",
        "    </main>\n",
        "\n",
        "    <footer>\n",
        "        <div class=\"container\">\n",
        "            <p>&copy; <span class=\"current-year\"></span> Bird Species Identifier Project</p>\n",
        "        </div>\n",
        "    </footer>\n",
        "\n",
        "    <script src=\"{{ url_for('static', filename='js/main.js') }}\"></script>\n",
        "    {% block scripts %}{% endblock %}\n",
        "</body>\n",
        "</html>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9ci4n3R0cy_",
        "outputId": "1bcb0770-0b32-4010-a331-4159b2a25ddf"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing BirdAudioProject/templates/layout.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create index.html\n",
        "%%writefile BirdAudioProject/templates/index.html\n",
        "\n",
        "{% extends \"layout.html\" %}\n",
        "\n",
        "{% block content %}\n",
        "<section class=\"hero\">\n",
        "    <h2>Identify Bird Species from Audio</h2>\n",
        "    <p>Upload a bird call or song recording to identify the species using our machine learning model.</p>\n",
        "</section>\n",
        "\n",
        "<section class=\"upload-section\">\n",
        "    <div class=\"upload-container\">\n",
        "        <h3>Upload Audio File</h3>\n",
        "        <form id=\"upload-form\" enctype=\"multipart/form-data\">\n",
        "            <div class=\"file-input\">\n",
        "                <input type=\"file\" id=\"audio-file\" name=\"audio_file\" accept=\".wav,.mp3,.ogg\">\n",
        "                <label for=\"audio-file\">Choose a file</label>\n",
        "                <span id=\"file-name\">No file chosen</span>\n",
        "            </div>\n",
        "            <div class=\"audio-controls\">\n",
        "                <audio id=\"audio-player\" controls style=\"display: none;\"></audio>\n",
        "            </div>\n",
        "            <button type=\"submit\" id=\"identify-btn\">Identify Bird</button>\n",
        "        </form>\n",
        "        <div id=\"loading\" style=\"display: none;\">\n",
        "            <div class=\"spinner\"></div>\n",
        "            <p>Analyzing audio...</p>\n",
        "        </div>\n",
        "    </div>\n",
        "</section>\n",
        "\n",
        "<section class=\"results-section\" id=\"results-section\" style=\"display: none;\">\n",
        "    <h3>Identification Results</h3>\n",
        "    <div class=\"results-container\">\n",
        "        <div class=\"bird-info\">\n",
        "            <div class=\"bird-image\">\n",
        "                <img id=\"bird-image\" src=\"/placeholder.svg\" alt=\"Bird Image\">\n",
        "            </div>\n",
        "            <div class=\"bird-details\">\n",
        "                <h4 id=\"bird-name\"></h4>\n",
        "                <p id=\"scientific-name\"></p>\n",
        "                <div class=\"confidence-meter\">\n",
        "                    <span>Confidence:</span>\n",
        "                    <div class=\"meter\">\n",
        "                        <div id=\"confidence-bar\"></div>\n",
        "                    </div>\n",
        "                    <span id=\"confidence-value\"></span>\n",
        "                </div>\n",
        "                <p id=\"bird-description\"></p>\n",
        "            </div>\n",
        "        </div>\n",
        "        <div class=\"audio-analysis\">\n",
        "            <h4>Audio Analysis</h4>\n",
        "            <div class=\"spectrogram\">\n",
        "                <img id=\"spectrogram\" src=\"/placeholder.svg\" alt=\"Audio Spectrogram\">\n",
        "            </div>\n",
        "        </div>\n",
        "    </div>\n",
        "</section>\n",
        "{% endblock %}\n",
        "\n",
        "{% block scripts %}\n",
        "<script>\n",
        "document.addEventListener('DOMContentLoaded', function() {\n",
        "    const uploadForm = document.getElementById('upload-form');\n",
        "    const audioFileInput = document.getElementById('audio-file');\n",
        "    const fileNameSpan = document.getElementById('file-name');\n",
        "    const audioPlayer = document.getElementById('audio-player');\n",
        "    const loadingDiv = document.getElementById('loading');\n",
        "    const resultsSection = document.getElementById('results-section');\n",
        "\n",
        "    // Update file name display when file is selected\n",
        "    audioFileInput.addEventListener('change', function() {\n",
        "        if (this.files && this.files[0]) {\n",
        "            const file = this.files[0];\n",
        "            fileNameSpan.textContent = file.name;\n",
        "\n",
        "            // Display audio player for preview\n",
        "            audioPlayer.src = URL.createObjectURL(file);\n",
        "            audioPlayer.style.display = 'block';\n",
        "        } else {\n",
        "            fileNameSpan.textContent = 'No file chosen';\n",
        "            audioPlayer.style.display = 'none';\n",
        "        }\n",
        "    });\n",
        "\n",
        "    // Handle form submission\n",
        "    uploadForm.addEventListener('submit', function(e) {\n",
        "        e.preventDefault();\n",
        "\n",
        "        const formData = new FormData(uploadForm);\n",
        "\n",
        "        if (!audioFileInput.files || audioFileInput.files.length === 0) {\n",
        "            alert('Please select an audio file first.');\n",
        "            return;\n",
        "        }\n",
        "\n",
        "        // Show loading spinner\n",
        "        loadingDiv.style.display = 'flex';\n",
        "        resultsSection.style.display = 'none';\n",
        "\n",
        "        // Send request to server\n",
        "        fetch('/identify', {\n",
        "            method: 'POST',\n",
        "            body: formData\n",
        "        })\n",
        "        .then(response => {\n",
        "            if (!response.ok) {\n",
        "                return response.json().then(data => {\n",
        "                    throw new Error(data.error || 'Error processing request');\n",
        "                });\n",
        "            }\n",
        "            return response.json();\n",
        "        })\n",
        "        .then(data => {\n",
        "            // Hide loading spinner\n",
        "            loadingDiv.style.display = 'none';\n",
        "\n",
        "            // Display results\n",
        "            document.getElementById('bird-name').textContent = data.common_name;\n",
        "            document.getElementById('scientific-name').textContent = data.scientific_name;\n",
        "            document.getElementById('bird-description').textContent = data.description;\n",
        "            document.getElementById('bird-image').src = data.image;\n",
        "\n",
        "            // Set confidence bar\n",
        "            const confidencePercent = Math.round(data.confidence * 100);\n",
        "            document.getElementById('confidence-bar').style.width = confidencePercent + '%';\n",
        "            document.getElementById('confidence-value').textContent = confidencePercent + '%';\n",
        "\n",
        "            // Set spectrogram\n",
        "            document.getElementById('spectrogram').src = 'data:image/png;base64,' + data.spectrogram;\n",
        "\n",
        "            // Show results section\n",
        "            resultsSection.style.display = 'block';\n",
        "        })\n",
        "        .catch(error => {\n",
        "            // Hide loading spinner\n",
        "            loadingDiv.style.display = 'none';\n",
        "            alert('Error: ' + error.message);\n",
        "        });\n",
        "    });\n",
        "});\n",
        "</script>\n",
        "{% endblock %}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7kn8nLR0iB-",
        "outputId": "7ec4595e-a307-453c-f3d1-cfc2341e44e0"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing BirdAudioProject/templates/index.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create about.html\n",
        "%%writefile BirdAudioProject/templates/about.html\n",
        "\n",
        "{% extends \"layout.html\" %}\n",
        "\n",
        "{% block content %}\n",
        "<section class=\"about-section\">\n",
        "    <h2>About Bird Species Identifier</h2>\n",
        "    <p>The Bird Species Identifier is a machine learning application that can identify bird species from audio recordings of their calls and songs.</p>\n",
        "\n",
        "    <div class=\"about-content\">\n",
        "        <div class=\"about-text\">\n",
        "            <h3>How It Works</h3>\n",
        "            <p>Our system uses a convolutional neural network (CNN) trained on a dataset of bird vocalizations. When you upload an audio file:</p>\n",
        "            <ol>\n",
        "                <li>The audio is processed to extract Mel-frequency cepstral coefficients (MFCCs)</li>\n",
        "                <li>These features are fed into our trained neural network</li>\n",
        "                <li>The model predicts the most likely bird species</li>\n",
        "                <li>We display the results along with information about the identified species</li>\n",
        "            </ol>\n",
        "\n",
        "            <h3>Technologies Used</h3>\n",
        "            <ul>\n",
        "                <li>Python for backend processing and machine learning</li>\n",
        "                <li>TensorFlow and Keras for the neural network model</li>\n",
        "                <li>Librosa for audio processing</li>\n",
        "                <li>Flask for the web application</li>\n",
        "                <li>HTML, CSS, and JavaScript for the user interface</li>\n",
        "            </ul>\n",
        "        </div>\n",
        "\n",
        "        <div class=\"about-image\">\n",
        "            <img src=\"{{ url_for('static', filename='images/about-image.jpg') }}\" alt=\"Birds in natural habitat\">\n",
        "        </div>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"dataset-info\">\n",
        "        <h3>Dataset Information</h3>\n",
        "        <p>Our model was trained on a diverse dataset of bird vocalizations, including various species from different habitats and regions. The audio samples were collected from various sources including:</p>\n",
        "        <ul>\n",
        "            <li>Xeno-canto - A community database of bird sounds from around the world</li>\n",
        "            <li>Cornell Lab of Ornithology's Macaulay Library</li>\n",
        "            <li>Field recordings by ornithologists and bird enthusiasts</li>\n",
        "        </ul>\n",
        "    </div>\n",
        "</section>\n",
        "{% endblock %}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9KzZQo70leP",
        "outputId": "29cf680d-a9a5-439c-ca91-5f45a3fb3f07"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing BirdAudioProject/templates/about.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create style.css\n",
        "%%writefile BirdAudioProject/static/css/style.css\n",
        "\n",
        "/* Base styles */\n",
        ":root {\n",
        "    --primary-color: #4CAF50;\n",
        "    --secondary-color: #2E7D32;\n",
        "    --accent-color: #8BC34A;\n",
        "    --text-color: #333;\n",
        "    --light-text: #fff;\n",
        "    --background-color: #f5f5f5;\n",
        "    --card-background: #fff;\n",
        "    --border-color: #ddd;\n",
        "    --error-color: #f44336;\n",
        "    --success-color: #4CAF50;\n",
        "}\n",
        "\n",
        "* {\n",
        "    margin: 0;\n",
        "    padding: 0;\n",
        "    box-sizing: border-box;\n",
        "}\n",
        "\n",
        "body {\n",
        "    font-family: 'Roboto', sans-serif;\n",
        "    line-height: 1.6;\n",
        "    color: var(--text-color);\n",
        "    background-color: var(--background-color);\n",
        "}\n",
        "\n",
        ".container {\n",
        "    width: 90%;\n",
        "    max-width: 1200px;\n",
        "    margin: 0 auto;\n",
        "    padding: 0 15px;\n",
        "}\n",
        "\n",
        "/* Header styles */\n",
        "header {\n",
        "    background-color: var(--primary-color);\n",
        "    color: var(--light-text);\n",
        "    padding: 1rem 0;\n",
        "    box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);\n",
        "}\n",
        "\n",
        "header .container {\n",
        "    display: flex;\n",
        "    justify-content: space-between;\n",
        "    align-items: center;\n",
        "}\n",
        "\n",
        "header h1 {\n",
        "    font-size: 1.8rem;\n",
        "    font-weight: 500;\n",
        "}\n",
        "\n",
        "nav ul {\n",
        "    display: flex;\n",
        "    list-style: none;\n",
        "}\n",
        "\n",
        "nav ul li {\n",
        "    margin-left: 1.5rem;\n",
        "}\n",
        "\n",
        "nav ul li a {\n",
        "    color: var(--light-text);\n",
        "    text-decoration: none;\n",
        "    font-weight: 500;\n",
        "    transition: color 0.3s;\n",
        "}\n",
        "\n",
        "nav ul li a:hover {\n",
        "    color: var(--accent-color);\n",
        "}\n",
        "\n",
        "/* Main content styles */\n",
        "main {\n",
        "    padding: 2rem 0;\n",
        "    min-height: calc(100vh - 130px);\n",
        "}\n",
        "\n",
        "/* Hero section */\n",
        ".hero {\n",
        "    text-align: center;\n",
        "    padding: 2rem 0;\n",
        "    margin-bottom: 2rem;\n",
        "}\n",
        "\n",
        ".hero h2 {\n",
        "    font-size: 2.5rem;\n",
        "    margin-bottom: 1rem;\n",
        "    color: var(--secondary-color);\n",
        "}\n",
        "\n",
        ".hero p {\n",
        "    font-size: 1.2rem;\n",
        "    max-width: 800px;\n",
        "    margin: 0 auto;\n",
        "    color: #666;\n",
        "}\n",
        "\n",
        "/* Upload section */\n",
        ".upload-section {\n",
        "    margin-bottom: 3rem;\n",
        "}\n",
        "\n",
        ".upload-container {\n",
        "    background-color: var(--card-background);\n",
        "    border-radius: 8px;\n",
        "    padding: 2rem;\n",
        "    box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);\n",
        "    max-width: 600px;\n",
        "    margin: 0 auto;\n",
        "}\n",
        "\n",
        ".upload-container h3 {\n",
        "    margin-bottom: 1.5rem;\n",
        "    color: var(--secondary-color);\n",
        "    text-align: center;\n",
        "}\n",
        "\n",
        ".file-input {\n",
        "    margin-bottom: 1.5rem;\n",
        "    display: flex;\n",
        "    flex-direction: column;\n",
        "}\n",
        "\n",
        ".file-input input[type=\"file\"] {\n",
        "    display: none;\n",
        "}\n",
        "\n",
        ".file-input label {\n",
        "    background-color: var(--primary-color);\n",
        "    color: white;\n",
        "    padding: 10px 20px;\n",
        "    border-radius: 4px;\n",
        "    cursor: pointer;\n",
        "    text-align: center;\n",
        "    margin-bottom: 10px;\n",
        "    transition: background-color 0.3s;\n",
        "}\n",
        "\n",
        ".file-input label:hover {\n",
        "    background-color: var(--secondary-color);\n",
        "}\n",
        "\n",
        "#file-name {\n",
        "    font-size: 0.9rem;\n",
        "    color: #666;\n",
        "    text-align: center;\n",
        "}\n",
        "\n",
        ".audio-controls {\n",
        "    margin-bottom: 1.5rem;\n",
        "}\n",
        "\n",
        ".audio-controls audio {\n",
        "    width: 100%;\n",
        "}\n",
        "\n",
        "button {\n",
        "    background-color: var(--primary-color);\n",
        "    color: white;\n",
        "    border: none;\n",
        "    padding: 12px 24px;\n",
        "    border-radius: 4px;\n",
        "    cursor: pointer;\n",
        "    font-size: 1rem;\n",
        "    font-weight: 500;\n",
        "    width: 100%;\n",
        "    transition: background-color 0.3s;\n",
        "}\n",
        "\n",
        "button:hover {\n",
        "    background-color: var(--secondary-color);\n",
        "}\n",
        "\n",
        "/* Loading spinner */\n",
        "#loading {\n",
        "    display: flex;\n",
        "    flex-direction: column;\n",
        "    align-items: center;\n",
        "    justify-content: center;\n",
        "    margin-top: 1rem;\n",
        "}\n",
        "\n",
        ".spinner {\n",
        "    border: 4px solid rgba(0, 0, 0, 0.1);\n",
        "    border-radius: 50%;\n",
        "    border-top: 4px solid var(--primary-color);\n",
        "    width: 40px;\n",
        "    height: 40px;\n",
        "    animation: spin 1s linear infinite;\n",
        "    margin-bottom: 1rem;\n",
        "}\n",
        "\n",
        "@keyframes spin {\n",
        "    0% { transform: rotate(0deg); }\n",
        "    100% { transform: rotate(360deg); }\n",
        "}\n",
        "\n",
        "/* Results section */\n",
        ".results-section {\n",
        "    margin-top: 3rem;\n",
        "}\n",
        "\n",
        ".results-section h3 {\n",
        "    text-align: center;\n",
        "    margin-bottom: 1.5rem;\n",
        "    color: var(--secondary-color);\n",
        "}\n",
        "\n",
        ".results-container {\n",
        "    background-color: var(--card-background);\n",
        "    border-radius: 8px;\n",
        "    padding: 2rem;\n",
        "    box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);\n",
        "}\n",
        "\n",
        ".bird-info {\n",
        "    display: flex;\n",
        "    flex-wrap: wrap;\n",
        "    margin-bottom: 2rem;\n",
        "    gap: 2rem;\n",
        "}\n",
        "\n",
        ".bird-image {\n",
        "    flex: 1;\n",
        "    min-width: 250px;\n",
        "}\n",
        "\n",
        ".bird-image img {\n",
        "    width: 100%;\n",
        "    border-radius: 8px;\n",
        "    box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);\n",
        "}\n",
        "\n",
        ".bird-details {\n",
        "    flex: 2;\n",
        "    min-width: 300px;\n",
        "}\n",
        "\n",
        ".bird-details h4 {\n",
        "    font-size: 1.8rem;\n",
        "    margin-bottom: 0.5rem;\n",
        "    color: var(--secondary-color);\n",
        "}\n",
        "\n",
        "#scientific-name {\n",
        "    font-style: italic;\n",
        "    color: #666;\n",
        "    margin-bottom: 1rem;\n",
        "}\n",
        "\n",
        ".confidence-meter {\n",
        "    margin: 1.5rem 0;\n",
        "}\n",
        "\n",
        ".meter {\n",
        "    height: 20px;\n",
        "    background-color: #e0e0e0;\n",
        "    border-radius: 10px;\n",
        "    margin: 10px 0;\n",
        "    overflow: hidden;\n",
        "}\n",
        "\n",
        "#confidence-bar {\n",
        "    height: 100%;\n",
        "    background-color: var(--primary-color);\n",
        "    width: 0%;\n",
        "    transition: width 1s ease-in-out;\n",
        "}\n",
        "\n",
        "#confidence-value {\n",
        "    font-weight: bold;\n",
        "}\n",
        "\n",
        ".audio-analysis {\n",
        "    margin-top: 2rem;\n",
        "}\n",
        "\n",
        ".audio-analysis h4 {\n",
        "    margin-bottom: 1rem;\n",
        "    color: var(--secondary-color);\n",
        "}\n",
        "\n",
        ".spectrogram {\n",
        "    text-align: center;\n",
        "}\n",
        "\n",
        ".spectrogram img {\n",
        "    max-width: 100%;\n",
        "    border-radius: 8px;\n",
        "    box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);\n",
        "}\n",
        "\n",
        "/* About page styles */\n",
        ".about-section {\n",
        "    max-width: 900px;\n",
        "    margin: 0 auto;\n",
        "}\n",
        "\n",
        ".about-section h2 {\n",
        "    text-align: center;\n",
        "    margin-bottom: 1.5rem;\n",
        "    color: var(--secondary-color);\n",
        "}\n",
        "\n",
        ".about-content {\n",
        "    display: flex;\n",
        "    flex-wrap: wrap;\n",
        "    gap: 2rem;\n",
        "    margin: 2rem 0;\n",
        "}\n",
        "\n",
        ".about-text {\n",
        "    flex: 3;\n",
        "    min-width: 300px;\n",
        "}\n",
        "\n",
        ".about-image {\n",
        "    flex: 2;\n",
        "    min-width: 250px;\n",
        "}\n",
        "\n",
        ".about-image img {\n",
        "    width: 100%;\n",
        "    border-radius: 8px;\n",
        "    box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);\n",
        "}\n",
        "\n",
        ".about-section h3 {\n",
        "    color: var(--secondary-color);\n",
        "    margin: 1.5rem 0 1rem;\n",
        "}\n",
        "\n",
        ".about-section ul, .about-section ol {\n",
        "    margin-left: 1.5rem;\n",
        "    margin-bottom: 1.5rem;\n",
        "}\n",
        "\n",
        ".dataset-info {\n",
        "    background-color: var(--card-background);\n",
        "    border-radius: 8px;\n",
        "    padding: 1.5rem;\n",
        "    box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);\n",
        "    margin-top: 2rem;\n",
        "}\n",
        "\n",
        "/* Footer styles */\n",
        "footer {\n",
        "    background-color: var(--primary-color);\n",
        "    color: var(--light-text);\n",
        "    padding: 1rem 0;\n",
        "    text-align: center;\n",
        "}\n",
        "\n",
        "/* Responsive styles */\n",
        "@media (max-width: 768px) {\n",
        "    header h1 {\n",
        "        font-size: 1.5rem;\n",
        "    }\n",
        "\n",
        "    .hero h2 {\n",
        "        font-size: 2rem;\n",
        "    }\n",
        "\n",
        "    .bird-info {\n",
        "        flex-direction: column;\n",
        "    }\n",
        "\n",
        "    .bird-image, .bird-details {\n",
        "        width: 100%;\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDh4cVD20p7V",
        "outputId": "d3364ffb-5cc6-4e29-d66b-03db8ce64167"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing BirdAudioProject/static/css/style.css\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create main.js\n",
        "%%writefile BirdAudioProject/static/js/main.js\n",
        "\n",
        "// Main JavaScript file for Bird Species Identifier\n",
        "\n",
        "// Function to format date and time\n",
        "function formatDateTime(date) {\n",
        "  const options = {\n",
        "      year: 'numeric',\n",
        "      month: 'short',\n",
        "      day: 'numeric',\n",
        "      hour: '2-digit',\n",
        "      minute: '2-digit'\n",
        "  };\n",
        "  return date.toLocaleDateString('en-US', options);\n",
        "}\n",
        "\n",
        "// Function to validate file size\n",
        "function validateFileSize(file, maxSizeMB) {\n",
        "  const maxSizeBytes = maxSizeMB * 1024 * 1024;\n",
        "  if (file.size > maxSizeBytes) {\n",
        "      return false;\n",
        "  }\n",
        "  return true;\n",
        "}\n",
        "\n",
        "// Add current year to footer\n",
        "document.addEventListener('DOMContentLoaded', function() {\n",
        "  // Replace any elements with the 'current-year' class with the current year\n",
        "  const yearElements = document.querySelectorAll('.current-year');\n",
        "  const currentYear = new Date().getFullYear();\n",
        "\n",
        "  yearElements.forEach(element => {\n",
        "      element.textContent = currentYear;\n",
        "  });\n",
        "});"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mfFYR8q0wJo",
        "outputId": "7548b9f2-6dd6-407f-912b-103d1c50bd6e"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing BirdAudioProject/static/js/main.js\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create bird_info.json\n",
        "%%writefile BirdAudioProject/static/data/bird_info.json\n",
        "\n",
        "{\n",
        "\"American_Robin\": {\n",
        "  \"common_name\": \"American Robin\",\n",
        "  \"scientific_name\": \"Turdus migratorius\",\n",
        "  \"description\": \"The American Robin is a migratory songbird of the true thrush genus and Turdidae, the wider thrush family. It is widely distributed throughout North America, wintering from southern Canada to central Mexico and along the Pacific Coast.\",\n",
        "  \"image\": \"/static/images/american_robin.jpg\"\n",
        "},\n",
        "\"Northern_Cardinal\": {\n",
        "  \"common_name\": \"Northern Cardinal\",\n",
        "  \"scientific_name\": \"Cardinalis cardinalis\",\n",
        "  \"description\": \"The Northern Cardinal is a bird in the genus Cardinalis. It is also known colloquially as the redbird, common cardinal, red cardinal, or just cardinal. It can be found in southeastern Canada, through the eastern United States from Maine to Minnesota to Texas, and south through Mexico, Belize, and Guatemala.\",\n",
        "  \"image\": \"/static/images/northern_cardinal.jpg\"\n",
        "},\n",
        "\"Blue_Jay\": {\n",
        "  \"common_name\": \"Blue Jay\",\n",
        "  \"scientific_name\": \"Cyanocitta cristata\",\n",
        "  \"description\": \"The Blue Jay is a passerine bird in the family Corvidae, native to eastern North America. It is resident through most of eastern and central United States and southern Canada, although western populations may be migratory.\",\n",
        "  \"image\": \"/static/images/blue_jay.jpg\"\n",
        "},\n",
        "\"Barn_Owl\": {\n",
        "  \"common_name\": \"Barn Owl\",\n",
        "  \"scientific_name\": \"Tyto alba\",\n",
        "  \"description\": \"The Barn Owl is the most widely distributed species of owl in the world and one of the most widespread of all species of birds. It is found almost everywhere in the world except for the polar and desert regions, Asia north of the Himalayas, most of Indonesia, and some Pacific islands.\",\n",
        "  \"image\": \"/static/images/barn_owl.jpg\"\n",
        "},\n",
        "\"Red_Tailed_Hawk\": {\n",
        "  \"common_name\": \"Red-tailed Hawk\",\n",
        "  \"scientific_name\": \"Buteo jamaicensis\",\n",
        "  \"description\": \"The Red-tailed Hawk is a bird of prey that breeds throughout most of North America, from the interior of Alaska and northern Canada to as far south as Panama and the West Indies. It is one of the most common members within the genus of Buteo in North America or worldwide.\",\n",
        "  \"image\": \"/static/images/red_tailed_hawk.jpg\"\n",
        "}\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sC36dm2x02Cd",
        "outputId": "d16d02f2-71a8-4fbe-83a3-6ed703bc71d2"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing BirdAudioProject/static/data/bird_info.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data_acquisition.py\n",
        "%%writefile BirdAudioProject/data_acquisition.py\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from tqdm import tqdm\n",
        "\n",
        "def download_xeno_canto_data(species_list, output_dir, max_recordings_per_species=10):\n",
        "    \"\"\"\n",
        "    Download bird audio recordings from Xeno-Canto API.\n",
        "\n",
        "    Args:\n",
        "        species_list (list): List of bird species to download\n",
        "        output_dir (str): Directory to save downloaded recordings\n",
        "        max_recordings_per_species (int): Maximum number of recordings per species\n",
        "    \"\"\"\n",
        "    base_url = \"https://www.xeno-canto.org/api/2/recordings\"\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for species in species_list:\n",
        "        print(f\"Downloading recordings for {species}...\")\n",
        "\n",
        "        # Create species directory\n",
        "        species_dir = os.path.join(output_dir, species.replace(\" \", \"_\"))\n",
        "        os.makedirs(species_dir, exist_ok=True)\n",
        "\n",
        "        # Query Xeno-Canto API\n",
        "        query = f\"?query={species}\"\n",
        "        response = requests.get(base_url + query)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            recordings = data.get('recordings', [])\n",
        "\n",
        "            # Limit number of recordings\n",
        "            recordings = recordings[:min(len(recordings), max_recordings_per_species)]\n",
        "\n",
        "            print(f\"Found {len(recordings)} recordings for {species}\")\n",
        "\n",
        "            # Download each recording\n",
        "            for i, recording in enumerate(recordings):\n",
        "                try:\n",
        "                    # Get download link\n",
        "                    file_url = recording.get('file')\n",
        "                    if not file_url:\n",
        "                        continue\n",
        "\n",
        "                    # Extract file name and extension\n",
        "                    file_name = f\"{species.replace(' ', '_')}_{i+1}.mp3\"\n",
        "                    file_path = os.path.join(species_dir, file_name)\n",
        "\n",
        "                    # Download file\n",
        "                    print(f\"Downloading {file_name}...\")\n",
        "                    audio_response = requests.get(file_url)\n",
        "\n",
        "                    if audio_response.status_code == 200:\n",
        "                        with open(file_path, 'wb') as f:\n",
        "                            f.write(audio_response.content)\n",
        "                        print(f\"Downloaded {file_name}\")\n",
        "                    else:\n",
        "                        print(f\"Failed to download {file_name}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error downloading recording: {e}\")\n",
        "        else:\n",
        "            print(f\"Failed to query Xeno-Canto API for {species}\")\n",
        "\n",
        "def generate_synthetic_data(output_dir, num_samples=5):\n",
        "    \"\"\"\n",
        "    Generate synthetic bird call data for testing when real data is not available.\n",
        "\n",
        "    Args:\n",
        "        output_dir (str): Directory to save synthetic data\n",
        "        num_samples (int): Number of samples per species\n",
        "    \"\"\"\n",
        "    # Bird species to simulate\n",
        "    species = [\"American_Robin\", \"Northern_Cardinal\", \"Blue_Jay\", \"Barn_Owl\", \"Red_Tailed_Hawk\"]\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Parameters for synthetic audio\n",
        "    sr = 22050  # Sample rate\n",
        "    duration = 3  # Duration in seconds\n",
        "\n",
        "    for bird in species:\n",
        "        # Create species directory\n",
        "        species_dir = os.path.join(output_dir, bird)\n",
        "        os.makedirs(species_dir, exist_ok=True)\n",
        "\n",
        "        print(f\"Generating synthetic data for {bird}...\")\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            # Generate synthetic bird call\n",
        "            # Each species has a different frequency range and pattern\n",
        "            if bird == \"American_Robin\":\n",
        "                # Robin-like chirps (higher frequency)\n",
        "                t = np.linspace(0, duration, int(sr * duration), endpoint=False)\n",
        "                chirp_rate = 4  # chirps per second\n",
        "                chirp_duration = 0.1\n",
        "                signal = np.zeros_like(t)\n",
        "\n",
        "                for j in range(int(duration * chirp_rate)):\n",
        "                    chirp_start = j / chirp_rate\n",
        "                    chirp_mask = (t >= chirp_start) & (t < chirp_start + chirp_duration)\n",
        "                    chirp = 0.5 * np.sin(2 * np.pi * 2000 * t[chirp_mask])\n",
        "                    signal[chirp_mask] = chirp\n",
        "\n",
        "            elif bird == \"Northern_Cardinal\":\n",
        "                # Cardinal-like whistles\n",
        "                t = np.linspace(0, duration, int(sr * duration), endpoint=False)\n",
        "                signal = 0.5 * np.sin(2 * np.pi * 1500 * t) * np.exp(-0.5 * ((t - 1.5) / 0.5) ** 2)\n",
        "\n",
        "            elif bird == \"Blue_Jay\":\n",
        "                # Jay-like harsh calls\n",
        "                t = np.linspace(0, duration, int(sr * duration), endpoint=False)\n",
        "                signal = 0.3 * np.sin(2 * np.pi * 1200 * t) + 0.2 * np.sin(2 * np.pi * 2400 * t)\n",
        "                signal *= (np.sin(2 * np.pi * 2 * t) > 0).astype(float)  # Add amplitude modulation\n",
        "\n",
        "            elif bird == \"Barn_Owl\":\n",
        "                # Owl-like screeches\n",
        "                t = np.linspace(0, duration, int(sr * duration), endpoint=False)\n",
        "                signal = 0.4 * np.sin(2 * np.pi * 800 * t) * np.exp(-0.5 * ((t - 1.5) / 1.0) ** 2)\n",
        "                # Add some noise\n",
        "                signal += 0.1 * np.random.normal(0, 1, len(t))\n",
        "\n",
        "            else:  # Red_Tailed_Hawk\n",
        "                # Hawk-like screams\n",
        "                t = np.linspace(0, duration, int(sr * duration), endpoint=False)\n",
        "                signal = 0.5 * np.sin(2 * np.pi * 600 * t) * np.exp(-0.5 * ((t - 1.5) / 0.8) ** 2)\n",
        "                # Add harmonics\n",
        "                signal += 0.3 * np.sin(2 * np.pi * 1200 * t) * np.exp(-0.5 * ((t - 1.5) / 0.8) ** 2)\n",
        "\n",
        "            # Add some background noise\n",
        "            noise = 0.05 * np.random.normal(0, 1, len(signal))\n",
        "            signal = signal + noise\n",
        "\n",
        "            # Normalize\n",
        "            signal = signal / np.max(np.abs(signal))\n",
        "\n",
        "            # Save as WAV file\n",
        "            file_path = os.path.join(species_dir, f\"{bird}_{i+1}.wav\")\n",
        "            sf.write(file_path, signal, sr)\n",
        "\n",
        "            print(f\"Generated {file_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage\n",
        "    species_list = [\n",
        "        \"American Robin\",\n",
        "        \"Northern Cardinal\",\n",
        "        \"Blue Jay\",\n",
        "        \"Barn Owl\",\n",
        "        \"Red-tailed Hawk\"\n",
        "    ]\n",
        "\n",
        "    # Uncomment to download real data from Xeno-Canto\n",
        "    # download_xeno_canto_data(species_list, \"data/bird_audio\")\n",
        "\n",
        "    # Generate synthetic data for testing\n",
        "    generate_synthetic_data(\"data/synthetic_bird_audio\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfzHnPMj0_MX",
        "outputId": "06a2bc1f-d4fb-4df9-eace-148f97cd7c46"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing BirdAudioProject/data_acquisition.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create test_model.py\n",
        "%%writefile BirdAudioProject/test_model.py\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from audio_processor import AudioProcessor\n",
        "import pickle\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "def test_model(model_path, encoder_path, test_data_dir):\n",
        "    \"\"\"\n",
        "    Test the trained model on test data.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): Path to the saved model\n",
        "        encoder_path (str): Path to the saved label encoder\n",
        "        test_data_dir (str): Directory containing test audio files organized by species\n",
        "    \"\"\"\n",
        "    # Load model\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "    # Load label encoder\n",
        "    with open(encoder_path, 'rb') as f:\n",
        "        label_encoder = pickle.load(f)\n",
        "\n",
        "    # Initialize audio processor\n",
        "    processor = AudioProcessor()\n",
        "\n",
        "    # Lists to store features and true labels\n",
        "    X_test = []\n",
        "    y_true = []\n",
        "    file_paths = []\n",
        "\n",
        "    # Process each species directory\n",
        "    species_dirs = [d for d in os.listdir(test_data_dir) if os.path.isdir(os.path.join(test_data_dir, d))]\n",
        "\n",
        "    for species in species_dirs:\n",
        "        species_dir = os.path.join(test_data_dir, species)\n",
        "        print(f\"Processing test data for {species}...\")\n",
        "\n",
        "        # Process each audio file in the species directory\n",
        "        audio_files = [f for f in os.listdir(species_dir) if f.endswith(('.wav', '.mp3', '.ogg'))]\n",
        "\n",
        "        for audio_file in audio_files:\n",
        "            file_path = os.path.join(species_dir, audio_file)\n",
        "\n",
        "            # Preprocess audio for model input\n",
        "            features = processor.preprocess_for_model(file_path)\n",
        "\n",
        "            if features is not None:\n",
        "                X_test.append(features[0])  # Remove batch dimension\n",
        "                y_true.append(species)\n",
        "                file_paths.append(file_path)\n",
        "\n",
        "    # Convert lists to arrays\n",
        "    X_test = np.array(X_test)\n",
        "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred_probs = model.predict(X_test)\n",
        "    y_pred_indices = np.argmax(y_pred_probs, axis=1)\n",
        "    y_pred = label_encoder.inverse_transform(y_pred_indices)\n",
        "\n",
        "    # Encode true labels\n",
        "    y_true_indices = label_encoder.transform(y_true)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = np.mean(y_pred == y_true)\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Print classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    cm = confusion_matrix(y_true_indices, y_pred_indices)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=label_encoder.classes_,\n",
        "                yticklabels=label_encoder.classes_)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save confusion matrix\n",
        "    plt.savefig('confusion_matrix.png')\n",
        "\n",
        "    # Print misclassified examples\n",
        "    print(\"\\nMisclassified Examples:\")\n",
        "    for i, (true, pred, file_path) in enumerate(zip(y_true, y_pred, file_paths)):\n",
        "        if true != pred:\n",
        "            print(f\"File: {os.path.basename(file_path)}\")\n",
        "            print(f\"True: {true}, Predicted: {pred}\")\n",
        "            print(f\"Probabilities: {y_pred_probs[i][y_pred_indices[i]]:.4f}\")\n",
        "            print()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage\n",
        "    model_path = \"models/bird_species_model.h5\"\n",
        "    encoder_path = \"models/label_encoder.pkl\"\n",
        "    test_data_dir = \"data/test_bird_audio\"\n",
        "\n",
        "    test_model(model_path, encoder_path, test_data_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjgTUngL1C6-",
        "outputId": "9efc4bba-1d26-411c-848d-4296cf7de3de"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing BirdAudioProject/test_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create README.md\n",
        "%%writefile BirdAudioProject/README.md\n",
        "\n",
        "# Bird Species Identification from Audio\n",
        "\n",
        "This project uses machine learning to identify bird species from audio recordings of their calls and songs. It includes a complete pipeline from audio processing to model training and a web interface for making predictions.\n",
        "\n",
        "## Features\n",
        "\n",
        "- Audio processing and feature extraction using librosa\n",
        "- Convolutional Neural Network (CNN) for bird species classification\n",
        "- Flask web application for uploading and analyzing bird audio\n",
        "- Visualization of audio spectrograms and model predictions\n",
        "- Synthetic data generation for testing when real data is unavailable\n",
        "\n",
        "## Project Structure\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3RZ5XKZ1Hmf",
        "outputId": "5dd25455-e2e1-417e-e23a-9520e9b46c39"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing BirdAudioProject/README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a modified training script\n",
        "%%writefile BirdAudioProject/train_with_real_data.py\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from audio_processor import AudioProcessor\n",
        "from model_trainer import ModelTrainer\n",
        "\n",
        "def train_model_with_data(data_dir, output_dir):\n",
        "    \"\"\"\n",
        "    Train a bird species identification model with audio data.\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): Directory containing audio files organized by species\n",
        "        output_dir (str): Directory to save model and results\n",
        "    \"\"\"\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Initialize audio processor\n",
        "    processor = AudioProcessor(sample_rate=22050, n_mfcc=13)\n",
        "\n",
        "    # Lists to store features and labels\n",
        "    features_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    # Process each species directory\n",
        "    species_dirs = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
        "\n",
        "    print(f\"Found {len(species_dirs)} species directories\")\n",
        "\n",
        "    for species in species_dirs:\n",
        "        species_dir = os.path.join(data_dir, species)\n",
        "        print(f\"Processing {species}...\")\n",
        "\n",
        "        # Process each audio file in the species directory\n",
        "        audio_files = [f for f in os.listdir(species_dir) if f.endswith(('.wav', '.mp3', '.ogg'))]\n",
        "\n",
        "        for audio_file in audio_files:\n",
        "            file_path = os.path.join(species_dir, audio_file)\n",
        "\n",
        "            # Load and preprocess audio\n",
        "            signal, sr = processor.load_audio(file_path)\n",
        "            if signal is None:\n",
        "                continue\n",
        "\n",
        "            # Extract features\n",
        "            features = processor.extract_features(signal, feature_type='mfcc')\n",
        "            if features is None:\n",
        "                continue\n",
        "\n",
        "            # Normalize features\n",
        "            normalized_features = processor.normalize_features(features)\n",
        "\n",
        "            # Ensure consistent feature length (e.g., 100 time steps)\n",
        "            fixed_length = 100\n",
        "            if normalized_features.shape[1] < fixed_length:\n",
        "                pad_width = fixed_length - normalized_features.shape[1]\n",
        "                normalized_features = np.pad(\n",
        "                    normalized_features,\n",
        "                    pad_width=((0, 0), (0, pad_width)),\n",
        "                    mode='constant'\n",
        "                )\n",
        "            else:\n",
        "                normalized_features = normalized_features[:, :fixed_length]\n",
        "\n",
        "            # Add to lists\n",
        "            features_list.append(normalized_features)\n",
        "            labels_list.append(species)\n",
        "\n",
        "    # Convert lists to arrays\n",
        "    X = np.array(features_list)\n",
        "    y = np.array(labels_list)\n",
        "\n",
        "    # Reshape features for CNN input: (samples, height, width, channels)\n",
        "    X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
        "\n",
        "    print(f\"Feature shape: {X.shape}\")\n",
        "    print(f\"Number of samples: {len(y)}\")\n",
        "    print(f\"Unique species: {np.unique(y)}\")\n",
        "\n",
        "    # Initialize and build model\n",
        "    input_shape = (X.shape[1], X.shape[2], 1)\n",
        "    trainer = ModelTrainer(input_shape=input_shape)\n",
        "\n",
        "    num_classes = len(np.unique(y))\n",
        "    model = trainer.build_model(num_classes)\n",
        "\n",
        "    # Train model\n",
        "    history = trainer.train(X, y, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "    # Plot and save training history\n",
        "    trainer.plot_training_history(save_path=os.path.join(output_dir, 'training_history.png'))\n",
        "\n",
        "    # Save model and encoder\n",
        "    model_path = os.path.join(output_dir, 'bird_species_model.h5')\n",
        "    encoder_path = os.path.join(output_dir, 'label_encoder.pkl')\n",
        "    trainer.save_model(model_path, encoder_path)\n",
        "\n",
        "    print(f\"Model saved to {model_path}\")\n",
        "    print(f\"Label encoder saved to {encoder_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Use the training data directory\n",
        "    data_dir = \"data/train_bird_audio\"\n",
        "    output_dir = \"models\"\n",
        "    train_model_with_data(data_dir, output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ny4AIhTq1LbG",
        "outputId": "a3506e62-b9ec-4e49-ce9e-6028613a4e83"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing BirdAudioProject/train_with_real_data.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Train the model with real data\n",
        "%cd BirdAudioProject\n",
        "!python train_with_real_data.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnMb3WNmB265",
        "outputId": "75b16aa1-40f5-434e-efe0-9e4cea5f9ef1"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/BirdAudioProject/BirdAudioProject/BirdAudioProject\n",
            "2025-04-09 14:46:48.110706: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744210008.210642   12554 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744210008.240266   12554 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-09 14:46:48.334579: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-04-09 14:46:57.868947: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "Model loaded successfully\n",
            "Label encoder loaded successfully\n",
            "WARNING:werkzeug: * Debugger is active!\n",
            "INFO:werkzeug: * Debugger PIN: 110-331-927\n",
            "Found 5 species directories\n",
            "Processing Red-tailed_Hawk...\n",
            "Processing Northern_Cardinal...\n",
            "Processing Barn_Owl...\n",
            "Processing Blue_Jay...\n",
            "Processing American_Robin...\n",
            "Feature shape: (40, 13, 100, 1)\n",
            "Number of samples: 40\n",
            "Unique species: ['American_Robin' 'Barn_Owl' 'Blue_Jay' 'Northern_Cardinal'\n",
            " 'Red-tailed_Hawk']\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "2025-04-09 14:47:15.106294: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
            "Epoch 1/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.2188 - loss: 2.1534 - val_accuracy: 0.2500 - val_loss: 1.6075\n",
            "Epoch 2/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.4375 - loss: 1.2664 - val_accuracy: 0.2500 - val_loss: 1.6171\n",
            "Epoch 3/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.5312 - loss: 1.1895 - val_accuracy: 0.2500 - val_loss: 1.6233\n",
            "Epoch 4/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - accuracy: 0.6875 - loss: 0.9161 - val_accuracy: 0.1250 - val_loss: 1.6238\n",
            "Epoch 5/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.7812 - loss: 0.6387 - val_accuracy: 0.1250 - val_loss: 1.6215\n",
            "Epoch 6/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294ms/step - accuracy: 0.9375 - loss: 0.4380 - val_accuracy: 0.0000e+00 - val_loss: 1.6196\n",
            "Epoch 7/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.8125 - loss: 0.5106 - val_accuracy: 0.0000e+00 - val_loss: 1.6152\n",
            "Epoch 8/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - accuracy: 0.9688 - loss: 0.3285 - val_accuracy: 0.0000e+00 - val_loss: 1.6107\n",
            "Epoch 9/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.9375 - loss: 0.2923 - val_accuracy: 0.1250 - val_loss: 1.6063\n",
            "Epoch 10/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.9688 - loss: 0.2468 - val_accuracy: 0.1250 - val_loss: 1.6018\n",
            "Epoch 11/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.9688 - loss: 0.2012 - val_accuracy: 0.1250 - val_loss: 1.5994\n",
            "Epoch 12/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 1.0000 - loss: 0.1323 - val_accuracy: 0.1250 - val_loss: 1.5995\n",
            "Epoch 13/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 288ms/step - accuracy: 1.0000 - loss: 0.0965 - val_accuracy: 0.1250 - val_loss: 1.5986\n",
            "Epoch 14/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy: 1.0000 - loss: 0.0838 - val_accuracy: 0.1250 - val_loss: 1.5976\n",
            "Epoch 15/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296ms/step - accuracy: 0.9688 - loss: 0.1378 - val_accuracy: 0.2500 - val_loss: 1.5966\n",
            "Epoch 16/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314ms/step - accuracy: 1.0000 - loss: 0.0791 - val_accuracy: 0.2500 - val_loss: 1.5956\n",
            "Epoch 17/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330ms/step - accuracy: 1.0000 - loss: 0.1008 - val_accuracy: 0.3750 - val_loss: 1.5937\n",
            "Epoch 18/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 1.0000 - loss: 0.0562 - val_accuracy: 0.3750 - val_loss: 1.5926\n",
            "Epoch 19/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 1.0000 - loss: 0.0376 - val_accuracy: 0.3750 - val_loss: 1.5922\n",
            "Epoch 20/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 616ms/step - accuracy: 1.0000 - loss: 0.0617 - val_accuracy: 0.3750 - val_loss: 1.5913\n",
            "Epoch 21/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 598ms/step - accuracy: 1.0000 - loss: 0.0293 - val_accuracy: 0.3750 - val_loss: 1.5894\n",
            "Epoch 22/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311ms/step - accuracy: 1.0000 - loss: 0.0288 - val_accuracy: 0.3750 - val_loss: 1.5877\n",
            "Epoch 23/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step - accuracy: 1.0000 - loss: 0.0315 - val_accuracy: 0.3750 - val_loss: 1.5863\n",
            "Epoch 24/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337ms/step - accuracy: 0.9375 - loss: 0.0945 - val_accuracy: 0.3750 - val_loss: 1.5842\n",
            "Epoch 25/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300ms/step - accuracy: 1.0000 - loss: 0.0226 - val_accuracy: 0.3750 - val_loss: 1.5818\n",
            "Epoch 26/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 1.0000 - loss: 0.0283 - val_accuracy: 0.3750 - val_loss: 1.5797\n",
            "Epoch 27/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 1.0000 - loss: 0.0275 - val_accuracy: 0.3750 - val_loss: 1.5772\n",
            "Epoch 28/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 1.0000 - loss: 0.0150 - val_accuracy: 0.3750 - val_loss: 1.5746\n",
            "Epoch 29/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 1.0000 - loss: 0.0196 - val_accuracy: 0.3750 - val_loss: 1.5717\n",
            "Epoch 30/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292ms/step - accuracy: 1.0000 - loss: 0.0214 - val_accuracy: 0.3750 - val_loss: 1.5694\n",
            "Epoch 31/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 1.0000 - loss: 0.0133 - val_accuracy: 0.3750 - val_loss: 1.5677\n",
            "Epoch 32/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302ms/step - accuracy: 1.0000 - loss: 0.0241 - val_accuracy: 0.5000 - val_loss: 1.5664\n",
            "Epoch 33/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - accuracy: 1.0000 - loss: 0.0353 - val_accuracy: 0.5000 - val_loss: 1.5646\n",
            "Epoch 34/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291ms/step - accuracy: 1.0000 - loss: 0.0323 - val_accuracy: 0.5000 - val_loss: 1.5621\n",
            "Epoch 35/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 297ms/step - accuracy: 1.0000 - loss: 0.0117 - val_accuracy: 0.5000 - val_loss: 1.5597\n",
            "Epoch 36/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307ms/step - accuracy: 1.0000 - loss: 0.0132 - val_accuracy: 0.5000 - val_loss: 1.5574\n",
            "Epoch 37/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 1.0000 - loss: 0.0115 - val_accuracy: 0.5000 - val_loss: 1.5553\n",
            "Epoch 38/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 1.0000 - loss: 0.0052 - val_accuracy: 0.5000 - val_loss: 1.5534\n",
            "Epoch 39/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 1.0000 - loss: 0.0075 - val_accuracy: 0.5000 - val_loss: 1.5516\n",
            "Epoch 40/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 0.5000 - val_loss: 1.5496\n",
            "Epoch 41/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 297ms/step - accuracy: 1.0000 - loss: 0.0033 - val_accuracy: 0.5000 - val_loss: 1.5476\n",
            "Epoch 42/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 1.0000 - loss: 0.0085 - val_accuracy: 0.5000 - val_loss: 1.5459\n",
            "Epoch 43/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 1.0000 - loss: 0.0036 - val_accuracy: 0.5000 - val_loss: 1.5446\n",
            "Epoch 44/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 1.0000 - loss: 0.0043 - val_accuracy: 0.5000 - val_loss: 1.5432\n",
            "Epoch 45/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 1.0000 - loss: 0.0040 - val_accuracy: 0.5000 - val_loss: 1.5417\n",
            "Epoch 46/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 1.0000 - loss: 0.0048 - val_accuracy: 0.5000 - val_loss: 1.5403\n",
            "Epoch 47/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 1.0000 - loss: 0.0035 - val_accuracy: 0.3750 - val_loss: 1.5391\n",
            "Epoch 48/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.3750 - val_loss: 1.5382\n",
            "Epoch 49/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step - accuracy: 1.0000 - loss: 0.0116 - val_accuracy: 0.3750 - val_loss: 1.5376\n",
            "Epoch 50/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 0.3750 - val_loss: 1.5372\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "Model saved to models/bird_species_model.h5\n",
            "Label encoder saved to models/label_encoder.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask pyngrok\n"
      ],
      "metadata": {
        "id": "5LQB9PODrEnL",
        "outputId": "54e6c83c-8c71-4ed8-c515-a43f07ab6c10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !ngrok config add-authtoken 2uo5kJSJJTKVT5hTWceu9gv0JDT_36trhvECETpBAFYespywL\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Add ngrok token (only need to do once per Colab session)\n",
        "!ngrok config add-authtoken 2uo5kJSJJTKVT5hTWceu9gv0JDT_36trhvECETpBAFYespywL\n",
        "\n"
      ],
      "metadata": {
        "id": "1TU5dL-Woc9a",
        "outputId": "ad810e5d-b416-466a-f4d0-2bfa107a0e63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask-ngrok pyngrok\n",
        "!ngrok authtoken 2uo5kJSJJTKVT5hTWceu9gv0JDT_36trhvECETpBAFYespywL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C28jOgfe1-sN",
        "outputId": "4e6b4929-4710-4b0a-fff2-9c72075cce65"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask-ngrok in /usr/local/lib/python3.11/dist-packages (0.0.25)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.11/dist-packages (from flask-ngrok) (3.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from flask-ngrok) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (1.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask>=0.8->flask-ngrok) (3.0.2)\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p BirdAudioProject\n"
      ],
      "metadata": {
        "id": "GAU6SU7MvBwq"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create app.py\n",
        "%%writefile BirdAudioProject/app.py\n",
        "\n",
        "# app.py\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from flask import Flask, render_template, request, jsonify\n",
        "import tensorflow as tf\n",
        "from audio_processor import AudioProcessor\n",
        "import pickle\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import base64\n",
        "from werkzeug.utils import secure_filename\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Configuration\n",
        "MODEL_PATH = 'models/bird_species_model.h5'\n",
        "ENCODER_PATH = 'models/label_encoder.pkl'\n",
        "UPLOAD_FOLDER = 'static/uploads'\n",
        "ALLOWED_EXTENSIONS = {'wav', 'mp3', 'ogg'}\n",
        "\n",
        "os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
        "processor = AudioProcessor()\n",
        "\n",
        "# Load bird info from JSON\n",
        "def load_bird_info():\n",
        "    try:\n",
        "        with open('static/data/bird_info.json', 'r') as f:\n",
        "            return json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading bird info: {e}\")\n",
        "        return {}\n",
        "\n",
        "bird_info = load_bird_info()\n",
        "\n",
        "# Load model and label encoder\n",
        "model, label_encoder = None, None\n",
        "\n",
        "def load_model_and_encoder():\n",
        "    global model, label_encoder\n",
        "    try:\n",
        "        if os.path.exists(MODEL_PATH):\n",
        "            model = tf.keras.models.load_model(MODEL_PATH)\n",
        "            print(\"Model loaded successfully\")\n",
        "        else:\n",
        "            print(f\"Model file not found at {MODEL_PATH}\")\n",
        "\n",
        "        if os.path.exists(ENCODER_PATH):\n",
        "            with open(ENCODER_PATH, 'rb') as f:\n",
        "                label_encoder = pickle.load(f)\n",
        "            print(\"Label encoder loaded successfully\")\n",
        "        else:\n",
        "            print(f\"Label encoder file not found at {ENCODER_PATH}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model/encoder: {e}\")\n",
        "\n",
        "load_model_and_encoder()\n",
        "\n",
        "# Helper to check file extension\n",
        "def allowed_file(filename):\n",
        "    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
        "\n",
        "# Routes\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return render_template('index.html')\n",
        "\n",
        "@app.route('/about')\n",
        "def about():\n",
        "    return render_template('about.html')\n",
        "\n",
        "@app.route('/identify', methods=['POST'])\n",
        "def identify_bird():\n",
        "    if 'audio_file' not in request.files:\n",
        "        return jsonify({'error': 'No file part'}), 400\n",
        "\n",
        "    file = request.files['audio_file']\n",
        "\n",
        "    if file.filename == '':\n",
        "        return jsonify({'error': 'No selected file'}), 400\n",
        "\n",
        "    if file and allowed_file(file.filename):\n",
        "        filename = secure_filename(file.filename)\n",
        "        file_path = os.path.join(UPLOAD_FOLDER, filename)\n",
        "        file.save(file_path)\n",
        "\n",
        "        if model is None or label_encoder is None:\n",
        "            load_model_and_encoder()\n",
        "            if model is None or label_encoder is None:\n",
        "                return jsonify({'error': 'Model not available'}), 500\n",
        "\n",
        "        try:\n",
        "            features = processor.preprocess_for_model(file_path)\n",
        "            if features is None:\n",
        "                return jsonify({'error': 'Failed to process audio file'}), 400\n",
        "\n",
        "            probabilities = model.predict(features)[0]\n",
        "            predicted_index = np.argmax(probabilities)\n",
        "            predicted_species = label_encoder.inverse_transform([predicted_index])[0]\n",
        "            confidence = float(probabilities[predicted_index])\n",
        "\n",
        "            y, sr = librosa.load(file_path)\n",
        "            plt.figure(figsize=(10, 4))\n",
        "            S = librosa.feature.melspectrogram(y=y, sr=sr)\n",
        "            librosa.display.specshow(librosa.power_to_db(S, ref=np.max), y_axis='mel', x_axis='time')\n",
        "            plt.colorbar(format='%+2.0f dB')\n",
        "            plt.title('Mel spectrogram')\n",
        "            buf = io.BytesIO()\n",
        "            plt.savefig(buf, format='png')\n",
        "            buf.seek(0)\n",
        "            plt.close()\n",
        "            spectrogram_b64 = base64.b64encode(buf.read()).decode('utf-8')\n",
        "\n",
        "            bird_data = bird_info.get(predicted_species, {\n",
        "                'common_name': predicted_species,\n",
        "                'scientific_name': 'Unknown',\n",
        "                'description': 'No information available.',\n",
        "                'image': '/static/images/placeholder.jpg'\n",
        "            })\n",
        "\n",
        "            return jsonify({\n",
        "                'species': predicted_species,\n",
        "                'common_name': bird_data['common_name'],\n",
        "                'scientific_name': bird_data['scientific_name'],\n",
        "                'description': bird_data['description'],\n",
        "                'image': bird_data['image'],\n",
        "                'confidence': confidence,\n",
        "                'spectrogram': spectrogram_b64\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            return jsonify({'error': f'Error during prediction: {str(e)}'}), 500\n",
        "\n",
        "    return jsonify({'error': 'Invalid file type'}), 400\n",
        "\n",
        "@app.route('/train', methods=['POST'])\n",
        "def train_model_endpoint():\n",
        "    return jsonify({'message': 'Training feature is not implemented in this demo.'})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n"
      ],
      "metadata": {
        "id": "bGUbGWmWrmdI",
        "outputId": "94250e64-7c95-44be-8bd0-505df2cb5264",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing BirdAudioProject/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import threading\n",
        "\n",
        "# Create a tunnel to the Flask app\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\"Your app is running on:\", public_url)\n",
        "\n",
        "# Run the Flask app in background thread\n",
        "def run_flask():\n",
        "    !python3 app.py\n",
        "\n",
        "thread = threading.Thread(target=run_flask)\n",
        "thread.start()\n"
      ],
      "metadata": {
        "id": "5yzkeRXhrpdS",
        "outputId": "2ede0af3-86e9-4535-de45-a988f4413aa4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your app is running on: NgrokTunnel: \"https://ec6e-34-125-52-51.ngrok-free.app\" -> \"http://localhost:5000\"\n"
          ]
        }
      ]
    }
  ]
}